{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsgNKHUrEVwD",
        "outputId": "e45b985d-e919-45e8-ec41-5c5818bf6e5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nWha69KMZtA",
        "outputId": "5f6fa67c-ca96-464e-cfa4-68ffe5acf494"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package mock_corpus to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mock_corpus.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lZr8jGfDuJJ",
        "outputId": "669ba59f-74b9-4da3-e86d-24a137ee5598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n",
            "[('queen', 0.7698540687561035), ('monarch', 0.6843381524085999), ('throne', 0.6755736470222473), ('daughter', 0.6594556570053101), ('princess', 0.6520534157752991), ('prince', 0.6517034769058228), ('elizabeth', 0.6464517712593079), ('mother', 0.631171703338623), ('emperor', 0.6106470823287964), ('wife', 0.6098655462265015)]\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load a pretrained model (smaller = faster to load and run)\n",
        "wv_pretrained = api.load(\"glove-wiki-gigaword-100\")\n",
        "\n",
        "# Find most similar words\n",
        "print(wv_pretrained.most_similar(\n",
        "    positive=[\"king\", \"woman\"],\n",
        "    negative=[\"man\"]\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample corpus (replace with your real tokenized data)\n",
        "sentences = [\n",
        "    ['this', 'is', 'a', 'sample'],\n",
        "    ['word2vec', 'model', 'with', 'gensim'],\n",
        "    ['train', 'your', 'own', 'embeddings']\n",
        "]\n",
        "\n",
        "custom_wv_model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    sg=1,                 # Skip-gram\n",
        "    vector_size=100,\n",
        "    window=3,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ").wv\n",
        "\n",
        "# Example usage\n",
        "print(custom_wv_model.most_similar('sample'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFqcAuBwEwNV",
        "outputId": "b849fc47-bd34-45e4-acfc-1cda66aba648"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('your', 0.19912061095237732), ('train', 0.07497556507587433), ('with', 0.060591842979192734), ('a', 0.044689226895570755), ('this', 0.0377129502594471), ('own', 0.03364058583974838), ('embeddings', 0.027057481929659843), ('is', 0.026806797832250595), ('model', 0.008826158009469509), ('gensim', -0.06900332123041153)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Example tokenized corpus\n",
        "sentences = [\n",
        "    ['hello', 'how', 'are', 'you'],\n",
        "    ['fasttext', 'supports', 'subwords'],\n",
        "    ['it', 'can', 'handle', 'rare', 'words'],\n",
        "]\n",
        "\n",
        "custom_fasttext_model = FastText(\n",
        "    sentences=sentences,\n",
        "    sg=1,                # Skip-gram model\n",
        "    vector_size=100,     # Embedding dimensions\n",
        "    window=3,            # Context window size\n",
        "    min_count=1,         # Minimum word frequency\n",
        "    workers=4            # Number of CPU cores to use\n",
        ").wv                    # .wv gives access to trained word vectors\n",
        "\n",
        "# Get vector for a word\n",
        "print(custom_fasttext_model['fasttext'])\n",
        "\n",
        "# Most similar words\n",
        "print(custom_fasttext_model.most_similar('handle'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8FI7Q4vF1Ow",
        "outputId": "cd7f862f-c82c-4353-c59c-d2f06c746bfc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9.3540893e-04 -4.5109953e-04 -4.7258567e-04  1.1702325e-04\n",
            "  6.8344036e-04  9.3575963e-04 -1.6304342e-03 -7.8891829e-04\n",
            " -1.6014793e-04 -2.1790760e-03 -2.6507976e-03  4.3747813e-04\n",
            "  4.9890886e-04  1.2042996e-03  6.4096670e-04 -2.3866985e-03\n",
            "  3.5789845e-04  1.1047089e-03  5.8790960e-04  7.0283201e-04\n",
            " -7.3491700e-04 -1.9162153e-03  1.8020131e-03  1.6453771e-04\n",
            "  1.9796486e-05  9.4133808e-05  1.2195419e-04 -2.5218099e-04\n",
            " -8.8847918e-04 -2.5623353e-04 -1.3378292e-04 -1.9213760e-04\n",
            " -8.2293933e-04  1.0338301e-03 -2.6749091e-03  3.7705048e-04\n",
            "  7.4897823e-04 -2.1760969e-04  9.6771267e-04  1.8948170e-03\n",
            " -4.3715438e-04  4.1976670e-04 -9.9141046e-04  1.3825184e-03\n",
            "  1.4196739e-03 -1.3829478e-03  3.6268061e-04  5.3091699e-05\n",
            " -6.1812613e-04  2.3007310e-04 -1.5061080e-03 -6.8651186e-04\n",
            "  5.6824129e-04  3.8750278e-04 -9.4411743e-04  9.0261636e-04\n",
            "  3.3747259e-04 -3.1186242e-03  2.5791084e-04  1.2504817e-03\n",
            " -2.8321848e-04 -8.9766982e-04 -2.1613284e-03 -9.9776790e-04\n",
            " -7.3052931e-04  1.9624314e-04 -3.3787565e-04  7.7898469e-04\n",
            "  9.2479202e-04 -1.2280075e-03 -7.2880671e-04  3.5555222e-05\n",
            " -2.9399508e-04 -1.2342014e-03  3.2666121e-03 -1.5253306e-03\n",
            "  2.6776409e-03 -1.0148858e-03  3.1105353e-04  1.5270413e-03\n",
            " -1.3228150e-03  9.0139057e-04 -1.4659122e-03  3.6727119e-04\n",
            " -1.0612431e-03 -1.2507273e-03 -9.7577344e-04 -8.4604736e-04\n",
            "  1.2329608e-03  1.6568804e-03  8.9856336e-04 -2.1235480e-03\n",
            " -1.5673215e-03  5.6435261e-04 -2.0050173e-04 -2.1819153e-04\n",
            " -1.8561023e-04 -4.3946956e-04 -9.5445482e-04 -2.5423360e-04]\n",
            "[('supports', 0.11179180443286896), ('you', 0.10007455945014954), ('it', 0.09893094003200531), ('rare', 0.061576616019010544), ('words', 0.02613251283764839), ('fasttext', 0.002967819105833769), ('are', -0.02760797180235386), ('can', -0.03154285252094269), ('subwords', -0.04319797456264496), ('hello', -0.06038514897227287)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part1"
      ],
      "metadata": {
        "id": "hR-fkmH-IkEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required packages\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load the pretrained Word2Vec model (this will take time and ~1.6GB RAM)\n",
        "print(\"Loading model... (this may take a few minutes)\")\n",
        "wv = api.load(\"word2vec-google-news-300\")\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smvfDPhRGK8n",
        "outputId": "3159a664-cd8a-491e-8858-de6e5df5a291"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model... (this may take a few minutes)\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"computer\", \"india\", \"football\", \"doctor\", \"music\"]\n",
        "\n",
        "for word in words:\n",
        "    print(f\"\\nTop 5 similar words to '{word}':\")\n",
        "    for similar_word, similarity in wv.most_similar(word, topn=5):\n",
        "        print(f\"  {similar_word} ({similarity:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C05-CTaQIpPj",
        "outputId": "ceaf66bc-691c-43aa-d945-1213fd263640"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 similar words to 'computer':\n",
            "  computers (0.7979)\n",
            "  laptop (0.6640)\n",
            "  laptop_computer (0.6549)\n",
            "  Computer (0.6473)\n",
            "  com_puter (0.6082)\n",
            "\n",
            "Top 5 similar words to 'india':\n",
            "  indian (0.6967)\n",
            "  usa (0.6836)\n",
            "  pakistan (0.6815)\n",
            "  chennai (0.6676)\n",
            "  america (0.6589)\n",
            "\n",
            "Top 5 similar words to 'football':\n",
            "  soccer (0.7314)\n",
            "  fooball (0.7140)\n",
            "  Football (0.7125)\n",
            "  basketball (0.6682)\n",
            "  footbal (0.6649)\n",
            "\n",
            "Top 5 similar words to 'doctor':\n",
            "  physician (0.7806)\n",
            "  doctors (0.7477)\n",
            "  gynecologist (0.6948)\n",
            "  surgeon (0.6793)\n",
            "  dentist (0.6785)\n",
            "\n",
            "Top 5 similar words to 'music':\n",
            "  classical_music (0.7198)\n",
            "  jazz (0.6835)\n",
            "  Music (0.6596)\n",
            "  Without_Donny_Kirshner (0.6416)\n",
            "  songs (0.6396)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: king - man + woman ≈ queen\n",
        "print(\"\\nking - man + woman:\")\n",
        "print(wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1))\n",
        "\n",
        "# Example 2: paris - france + italy ≈ rome\n",
        "print(\"\\nparis - france + italy:\")\n",
        "print(wv.most_similar(positive=[\"paris\", \"italy\"], negative=[\"france\"], topn=1))\n",
        "\n",
        "# Example 3: apple - iphone + android ≈ samsung\n",
        "print(\"\\napple - iphone + android:\")\n",
        "print(wv.most_similar(positive=[\"apple\", \"android\"], negative=[\"iphone\"], topn=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st-wNW70IxHZ",
        "outputId": "16db5114-91f8-43c1-cb12-4afc6fbf0e3a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "king - man + woman:\n",
            "[('queen', 0.7118193507194519)]\n",
            "\n",
            "paris - france + italy:\n",
            "[('lohan', 0.5069674849510193)]\n",
            "\n",
            "apple - iphone + android:\n",
            "[('apples', 0.48944351077079773)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part2"
      ],
      "metadata": {
        "id": "aixCIsZYI0C5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec, FastText\n",
        "import gensim.downloader as api\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ex-msBeLIxKP",
        "outputId": "8dfc3cad-7651-482b-8aa2-8f73a20f76ef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"IMDB Dataset.csv\")  # Make sure this CSV is present\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5TvHKyTXIxNG",
        "outputId": "820616ab-cd1d-4bd0-ae17-86d216affcf4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7cfacf6a-7b52-4b8c-8959-8f03fdae106b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7cfacf6a-7b52-4b8c-8959-8f03fdae106b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7cfacf6a-7b52-4b8c-8959-8f03fdae106b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7cfacf6a-7b52-4b8c-8959-8f03fdae106b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-99131340-3055-4f4a-8a84-3b58fc2705b2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-99131340-3055-4f4a-8a84-3b58fc2705b2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-99131340-3055-4f4a-8a84-3b58fc2705b2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['sentiment'].value_counts())\n",
        "df['review_length'] = df['review'].apply(lambda x: len(x.split()))\n",
        "df['review_length'].hist(bins=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "cj8GV6TkK8H7",
        "outputId": "621b8c68-3af0-4cdd-fdf6-de510095f847"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stop_words = set(stopwords.words('english'))\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# def clean_text(text):\n",
        "#     text = re.sub(r'<.*?>', '', text)\n",
        "#     text = text.lower()\n",
        "#     text = re.sub(r\"http\\S+|www\\S+|@\\S+|[^a-zA-Z]\", \" \", text)\n",
        "#     tokens = word_tokenize(text)\n",
        "#     tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
        "#     return tokens\n",
        "\n",
        "# df['tokens'] = df['review'].apply(clean_text)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+|@\\S+|[^a-zA-Z]\", \" \", text)  # Remove URLs and special characters\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
        "    return tokens\n",
        "\n",
        "# Apply to dataset\n",
        "df['tokens'] = df['review'].apply(clean_text)"
      ],
      "metadata": {
        "id": "8G6_vp3ZK8M2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_vector(model, doc):\n",
        "    doc = [word for word in doc if word in model]\n",
        "    return np.mean(model[doc], axis=0) if len(doc) > 0 else np.zeros(model.vector_size)"
      ],
      "metadata": {
        "id": "HiWJr40NK8P_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#🧠 Model 1: Pre-trained Word2Vec Vectors\n",
        "# Load pre-trained model\n",
        "print(\"Loading pretrained word2vec-google-news-300...\")\n",
        "w2v = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Vectorize\n",
        "X = np.array([document_vector(w2v, tokens) for tokens in df['tokens']])\n",
        "y = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"\\nPre-trained W2V Results:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJTo2mP3K8bN",
        "outputId": "2dfdc965-5bde-4b7e-fad8-1a4c0afc760a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained word2vec-google-news-300...\n",
            "\n",
            "Pre-trained W2V Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85      4961\n",
            "           1       0.85      0.85      0.85      5039\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#🧠 Model 2: Custom Word2Vec Skip-gram\n",
        "sg_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, sg=1, min_count=2, workers=4)\n",
        "X_sg = np.array([document_vector(sg_model.wv, tokens) for tokens in df['tokens']])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sg, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"\\nCustom Word2Vec (Skip-gram) Results:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TH-Gu-owLQ-k",
        "outputId": "329891e7-d903-4665-f0be-5fc4f440fa19"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Word2Vec (Skip-gram) Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87      4961\n",
            "           1       0.87      0.87      0.87      5039\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#🧠 Model 3: Custom Word2Vec CBoW\n",
        "cbow_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, sg=0, min_count=2, workers=4)\n",
        "X_cbow = np.array([document_vector(cbow_model.wv, tokens) for tokens in df['tokens']])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cbow, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"\\nCustom Word2Vec (CBoW) Results:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0OcGwWoLRBs",
        "outputId": "acbe80ae-1d12-45e5-f025-d597ec8db653"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Word2Vec (CBoW) Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.87      4961\n",
            "           1       0.86      0.87      0.87      5039\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#🧠 Model 4: Custom FastText\n",
        "ft_model = FastText(sentences=df['tokens'], vector_size=100, window=5, min_count=2, workers=4)\n",
        "X_ft = np.array([document_vector(ft_model.wv, tokens) for tokens in df['tokens']])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ft, y, test_size=0.2, random_state=42)\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"\\nCustom FastText Results:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR3nH5ukLRE0",
        "outputId": "40a4296c-5d1a-4441-90b2-019960551293"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom FastText Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.85      4961\n",
            "           1       0.84      0.86      0.85      5039\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# import pandas as pd\n",
        "\n",
        "# # Assuming you have:\n",
        "# # X_train, X_test, y_train, y_test for pretrained Word2Vec\n",
        "# # X_train_sg, X_test_sg for Skip-gram\n",
        "# # X_train_cbow, X_test_cbow for CBoW\n",
        "# # X_train_ft, X_test_ft for FastText\n",
        "\n",
        "# # Define and train separate models\n",
        "# model_pretrained = LogisticRegression(max_iter=1000)\n",
        "# model_pretrained.fit(X_train, y_train)\n",
        "\n",
        "# model_sg = LogisticRegression(max_iter=1000)\n",
        "# model_sg.fit(X_train_sg, y_train)\n",
        "\n",
        "# model_cbow = LogisticRegression(max_iter=1000)\n",
        "# model_cbow.fit(X_train_cbow, y_train)\n",
        "\n",
        "# model_ft = LogisticRegression(max_iter=1000)\n",
        "# model_ft.fit(X_train_ft, y_train)\n",
        "\n",
        "# # Evaluate each\n",
        "# acc_pretrained = accuracy_score(y_test, model_pretrained.predict(X_test))\n",
        "# acc_sg = accuracy_score(y_test, model_sg.predict(X_test_sg))\n",
        "# acc_cbow = accuracy_score(y_test, model_cbow.predict(X_test_cbow))\n",
        "# acc_ft = accuracy_score(y_test, model_ft.predict(X_test_ft))\n",
        "\n",
        "# # Store in dataframe\n",
        "# results = pd.DataFrame({\n",
        "#     \"Model\": [\n",
        "#         \"Pretrained Word2Vec (Google News)\",\n",
        "#         \"Custom Word2Vec (Skip-gram)\",\n",
        "#         \"Custom Word2Vec (CBoW)\",\n",
        "#         \"Custom FastText\"\n",
        "#     ],\n",
        "#     \"Accuracy\": [\n",
        "#         acc_pretrained,\n",
        "#         acc_sg,\n",
        "#         acc_cbow,\n",
        "#         acc_ft\n",
        "#     ]\n",
        "# })\n",
        "\n",
        "# print(results)"
      ],
      "metadata": {
        "id": "RKec6YrKLeyz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4I_rkD2SP3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}